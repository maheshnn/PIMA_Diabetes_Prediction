{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ab957c",
   "metadata": {},
   "source": [
    "A healthcare clinic is looking to enhance its decision-making process for predicting the likelihood of diabetes in patients based on a set of health-related attributes. Diabetes is a growing concern, especially among certain age groups and populations with risk factors such as high blood pressure, obesity, and a sedentary lifestyle. The clinic has collected patient data, including metrics like plasma glucose concentration, blood pressure, body mass index (BMI), and others, which can be used to predict diabetes. However, they struggle with accurately identifying high-risk patients who may need further medical intervention or lifestyle changes.\n",
    "\n",
    "The clinic seeks to leverage machine learning models to predict diabetes risk and ultimately improve patient care. By accurately predicting the probability of a patient having diabetes based on their health metrics, the clinic aims to provide early diagnosis, enabling timely intervention and better healthcare outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9089d",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "You have been hired as a data scientist to help the clinic build a predictive model that can classify whether or not a patient is likely to have diabetes based on their health attributes. Your goal is to develop a machine learning pipeline using a Gradient Boosting classifier to analyze patient data and predict diabetes outcomes (class label 1: diabetes, 0: no diabetes). The outcome should involve deploying this model and enable real-time predictions for clinical use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd9253",
   "metadata": {},
   "source": [
    "The dataset consists of health-related attributes of patients, with the following features:\n",
    "\n",
    "- Pregnancies (preg): Number of times the patient has been pregnant\n",
    "- Plasma glucose concentration (plas): Plasma glucose concentration in an - oral glucose tolerance test\n",
    "- Blood pressure (pres): Diastolic blood pressure (mm Hg)\n",
    "- Skin thickness (skin): Triceps skin fold thickness (mm)\n",
    "- Serum insulin (test): 2-Hour serum insulin (mu U/ml)\n",
    "- Body mass index (mass): BMI (weight in kg/height in mÂ²)\n",
    "- Diabetes pedigree function (pedi): A function that scores the likelihood of diabetes based on family history\n",
    "- Age (age): Age of the patient (years)\n",
    "- Class (class): Diabetes outcome (1: diabetes, 0: no diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07400bd1",
   "metadata": {},
   "source": [
    "**Note: When working with google colab environment, the local files will not be accessible. It acts as a VM and all the files created will be in colab server. We cannot see that, but if you run a shell command to list the folders we will be able to see it. To overcome that, in this notebook, we are using google drive to store the folders and the code. The google drive is added to the local PC. The google drive is also mounted on colab server.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e790872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mounting the google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98d219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops', exist_ok=True)\n",
    "os.makedirs(\"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/data\", exist_ok=True)\n",
    "# Create a folder for storing the model building files\n",
    "os.makedirs(\"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/model_building\", exist_ok=True)\n",
    "# This takes couple of minutes to reflect locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f868ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"maheshnn/PIMA-Diabetes-Prediction\"  # Hugging Face username\n",
    "print(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "393fdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/model_building/data_register.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/model_building/data_register.py\"\n",
    "from huggingface_hub.utils import RepositoryNotFoundError, HfHubHTTPError\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import os\n",
    "\n",
    "repo_id = \"maheshnn/PIMA-Diabetes-Prediction\" \n",
    "repo_type = \"dataset\"\n",
    "\n",
    "# Initialize API client\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "# Step 1: Check if the space exists\n",
    "try:\n",
    "    api.repo_info(repo_id=repo_id, repo_type=repo_type)\n",
    "    print(f\"Space '{repo_id}' already exists. Using it.\")\n",
    "except RepositoryNotFoundError:\n",
    "    print(f\"Space '{repo_id}' not found. Creating new space...\")\n",
    "    create_repo(repo_id=repo_id, repo_type=repo_type, private=False)\n",
    "    print(f\"Space '{repo_id}' created.\")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"self_paced_courses_1_mlops/data\", # Uploading the data \n",
    "    repo_id=repo_id,\n",
    "    repo_type=repo_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303bc60a",
   "metadata": {},
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc3438a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/model_building/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/model_building/prep.py\"\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "DATASET_PATH = \"hf://datasets/maheshnn/PIMA-Diabetes-Prediction/pima.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "target_col = 'class'\n",
    "\n",
    "# Split into X (features) and y (target)\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Perform train-test split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "Xtrain.to_csv(\"Xtrain.csv\",index=False)\n",
    "Xtest.to_csv(\"Xtest.csv\",index=False)\n",
    "ytrain.to_csv(\"ytrain.csv\",index=False)\n",
    "ytest.to_csv(\"ytest.csv\",index=False)\n",
    "\n",
    "\n",
    "files = [\"Xtrain.csv\",\"Xtest.csv\",\"ytrain.csv\",\"ytest.csv\"]\n",
    "\n",
    "\n",
    "for file_path in files:\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=file_path,\n",
    "        path_in_repo=file_path.split(\"/\")[-1],  # just the filename\n",
    "        repo_id=\"maheshnn/PIMA-Diabetes-Prediction\",                                    \n",
    "        repo_type=\"dataset\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc448c",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "574b8f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/model_building/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/model_building/train.py\"\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# For model training, tuning and evaluation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, recall_score\n",
    "\n",
    "# for model serialization\n",
    "import joblib\n",
    "\n",
    "# for Hugging face space authentication to upload files\n",
    "from huggingface_hub import login, HfApi, create_repo\n",
    "from huggingface_hub.utils import RepositoryNotFoundError, HfHubHTTPError\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "Xtrain_path = \"hf://datasets/maheshnn/PIMA-Diabetes-Prediction/Xtrain.csv\"                   \n",
    "Xtest_path = \"hf://datasets/maheshnn/PIMA-Diabetes-Prediction/Xtest.csv\"                      \n",
    "ytrain_path = \"hf://datasets/maheshnn/PIMA-Diabetes-Prediction/ytrain.csv\"                    \n",
    "ytest_path = \"hf://datasets/maheshnn/PIMA-Diabetes-Prediction/ytest.csv\"\n",
    "\n",
    "Xtrain = pd.read_csv(Xtrain_path)\n",
    "Xtest = pd.read_csv(Xtest_path)\n",
    "ytrain = pd.read_csv(ytrain_path)\n",
    "ytest = pd.read_csv(ytest_path)\n",
    "\n",
    "# scale numeric features\n",
    "numeric_features = [\n",
    "    'preg',\n",
    "    'plas',\n",
    "    'pres',\n",
    "    'skin',\n",
    "    'test',\n",
    "    'mass',\n",
    "    'pedi',\n",
    "    'age'\n",
    "]\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric_features)\n",
    ")\n",
    "\n",
    "# Define GB model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'gradientboostingclassifier__n_estimators': [75, 100, 125],\n",
    "    'gradientboostingclassifier__max_depth': [2, 3, 4],\n",
    "    'gradientboostingclassifier__subsample': [0.5, 0.6]\n",
    "}\n",
    "\n",
    "# Create pipeline\n",
    "model_pipeline = make_pipeline(preprocessor, gb_model)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(model_pipeline, param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
    "grid_search.fit(Xtrain, ytrain)\n",
    "\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Params:\\n\", grid_search.best_params_)\n",
    "\n",
    "# Predict on training set\n",
    "y_pred_train = best_model.predict(Xtrain)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_test = best_model.predict(Xtest)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(ytrain, y_pred_train))\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(ytest, y_pred_test))\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, \"best_pima_diabetes_model_v1.joblib\")\n",
    "\n",
    "# Upload to Hugging Face\n",
    "repo_id = \"maheshnn/PIMA-Diabetes-Prediction\"                                        \n",
    "repo_type = \"model\"\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "# Step 1: Check if the space exists\n",
    "try:\n",
    "    api.repo_info(repo_id=repo_id, repo_type=repo_type)\n",
    "    print(f\"Model Space '{repo_id}' already exists. Using it.\")\n",
    "except RepositoryNotFoundError:\n",
    "    print(f\"Model Space '{repo_id}' not found. Creating new space...\")\n",
    "    create_repo(repo_id=repo_id, repo_type=repo_type, private=False)\n",
    "    print(f\"Model Space '{repo_id}' created.\")\n",
    "\n",
    "# create_repo(\"best_machine_failure_model\", repo_type=\"model\", private=False)\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"best_pima_diabetes_model_v1.joblib\",\n",
    "    path_in_repo=\"best_pima_diabetes_model_v1.joblib\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=repo_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631dcbb",
   "metadata": {},
   "source": [
    "**Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7abc9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/deployment\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f76777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/deployment/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/deployment/Dockerfile\"\n",
    "# Use a minimal base image with Python 3.9 installed\n",
    "FROM python:3.9\n",
    "\n",
    "# Set the working directory inside the container to /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy all files from the current directory on the host to the container's /app directory\n",
    "COPY . .\n",
    "\n",
    "# Install Python dependencies listed in requirements.txt\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "RUN useradd -m -u 1000 user\n",
    "USER user\n",
    "ENV HOME=/home/user \\\n",
    "\tPATH=/home/user/.local/bin:$PATH\n",
    "\n",
    "WORKDIR $HOME/app\n",
    "\n",
    "COPY --chown=user . $HOME/app\n",
    "\n",
    "# Define the command to run the Streamlit app on port \"8501\" and make it accessible externally\n",
    "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73480bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/deployment/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/deployment/app.py\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "import joblib\n",
    "\n",
    "# Download and load the model\n",
    "model_path = hf_hub_download(repo_id=\"maheshnn/PIMA-Diabetes-Prediction\", filename=\"best_pima_diabetes_model_v1.joblib\")                                       # enter the Hugging Face username here\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Streamlit UI for Machine Failure Prediction\n",
    "st.title(\"PIMA Diabetes Prediction App\")\n",
    "st.write(\"\"\"\n",
    "This application predicts the likelihood of a patient having diabetes based on their health attributes.\n",
    "Please enter the sensor and configuration data below to get a prediction.\n",
    "\"\"\")\n",
    "\n",
    "# User inputs\n",
    "preg = st.number_input(\"Number of Pregnancies\", min_value=0, max_value=20, value=1)\n",
    "plas = st.number_input(\"Plasma Glucose Concentration\", min_value=0, max_value=300, value=120)\n",
    "pres = st.number_input(\"Diastolic Blood Pressure (mm Hg)\", min_value=0, max_value=200, value=70)\n",
    "skin = st.number_input(\"Triceps Skinfold Thickness (mm)\", min_value=0, max_value=100, value=20)\n",
    "test = st.number_input(\"2-Hour Serum Insulin (mu U/ml)\", min_value=0, max_value=900, value=80)\n",
    "mass = st.number_input(\"Body Mass Index (BMI)\", min_value=0.0, max_value=70.0, value=25.0, step=0.1)\n",
    "pedi = st.number_input(\"Diabetes Pedigree Function\", min_value=0.0, max_value=2.5, value=0.5, step=0.01)\n",
    "age = st.number_input(\"Age\", min_value=1, max_value=120, value=30)\n",
    "\n",
    "# Assemble input into DataFrame\n",
    "input_data = pd.DataFrame([{\n",
    "    'preg': preg,\n",
    "    'plas': plas,\n",
    "    'pres': pres,\n",
    "    'skin': skin,\n",
    "    'test': test,\n",
    "    'mass': mass,\n",
    "    'pedi': pedi,\n",
    "    'age': age\n",
    "}])\n",
    "\n",
    "# Prediction button\n",
    "if st.button(\"Predict Diabetes\"):\n",
    "    prediction = model.predict(input_data)[0]\n",
    "    result = \"Diabetic\" if prediction == 1 else \"Non-Diabetic\"\n",
    "    st.subheader(\"Prediction Result:\")\n",
    "    st.success(f\"The model predicts: **{result}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc3ab292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/deployment/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/deployment/requirements.txt\"\n",
    "pandas==2.2.2\n",
    "huggingface_hub==0.32.6\n",
    "streamlit==1.43.2\n",
    "joblib==1.5.1\n",
    "scikit-learn==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9ee0a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/hosting\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "266dc3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/hosting/hosting.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/hosting/hosting.py\"\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"self_paced_courses_1_mlops/deployment\",\n",
    "    repo_id=\"maheshnn/PIMA-Diabetes-Prediction\"                                       \n",
    "    repo_type=\"space\",\n",
    "    path_in_repo=\"\",                          # optional: subfolder path inside the repo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf284278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/self_paced_courses_1_mlops/requirements.txt\"\n",
    "huggingface_hub==0.32.6\n",
    "datasets==3.6.0\n",
    "pandas==2.2.2\n",
    "scikit-learn==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89740719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/.github/workflows\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8951d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/.github/workflows/pipeline.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"/content/drive/My Drive/PGP-AI-UT-Austin/Week11-MLOps/PIMA_Diabetes_Prediction/.github/workflows/pipeline.yml\"\n",
    "name: MLOps pipeline\n",
    "\n",
    "on:\n",
    "  workflow_dispatch:\n",
    "\n",
    "jobs:\n",
    "\n",
    "  register-dataset:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Install Dependencies\n",
    "        run: pip install -r self_paced_courses_1_mlops/requirements.txt\n",
    "      - name: Upload Dataset to Hugging Face Hub\n",
    "        env:\n",
    "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "        run: python self_paced_courses_1_mlops/model_building/data_register.py\n",
    "\n",
    "  data-prep:\n",
    "    needs: register-dataset\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Install Dependencies\n",
    "        run: pip install -r self_paced_courses_1_mlops/requirements.txt\n",
    "      - name: Run Data Preparation\n",
    "        env:\n",
    "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "        run: python self_paced_courses_1_mlops/model_building/prep.py\n",
    "\n",
    "\n",
    "  model-training:\n",
    "    needs: data-prep\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Install Dependencies\n",
    "        run: pip install -r self_paced_courses_1_mlops/requirements.txt\n",
    "      - name: Model Building\n",
    "        env:\n",
    "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "        run: python self_paced_courses_1_mlops/model_building/train.py\n",
    "\n",
    "\n",
    "  deploy-hosting:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: [model-training,data-prep,register-dataset]\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Install Dependencies\n",
    "        run: pip install -r self_paced_courses_1_mlops/requirements.txt\n",
    "      - name: Push files to Frontend Hugging Face Space\n",
    "        env:\n",
    "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "        run: python self_paced_courses_1_mlops/hosting/hosting.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
